#!/usr/bin/python3

import optparse
import socket
import re
import requests
import sys
import time
import os
import pyjs
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

white = "\033[1;37m"
green = '\033[1;32;48m'
bgreen = '\033[1;32m'
bred = '\033[1;31m'
noclr = '\033[0m'
bblue = '\033[1;34m'
cyan = '\033[0;36m'
orng = '\033[0;33m'

name = '''
|_ _|| |_  ___ | | | _ _ ._ _ _  ___ ._ _ / __> ___ <_> _| | ___  _ _
 | | | . |/ ._>|   || | || ' ' |<_> || ' |\__ \| . \| |/ . |/ ._>| '_>
 |_| |_|_|\___.|_|_|`___||_|_|_|<___||_|_|<___/|  _/|_|\___|\___.|_|
                                               |_|                   '''

print(f'''\n{bblue}{name}{noclr}{bred}v(1.1)-stable{noclr}
Author: {bred}z3r0day{noclr}
{white}Copyright (c) 2021{noclr}

{bgreen}[Spider the web]{noclr}\n''')

user_agent_headers = {
    'googlebot': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
	'chrome_win': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',
	'chrome_mac': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',
	'chrome_linux': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',
	'chrome_iphone': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/96.0.4664.36 Mobile/15E148 Safari/604.1',
	'chrome_android': 'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.50 Mobile Safari/537.36',
	'firefox_win': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0',
	'firefox_linux': 'Mozilla/5.0 (Linux x86_64; rv:94.0) Gecko/20100101 Firefox/94.0',
	'firefox_mac': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 12.0; rv:94.0) Gecko/20100101 Firefox/94.0',
	'firefox_iphone': 'Mozilla/5.0 (iPhone; CPU iPhone OS 12_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/39.0 Mobile/15E148 Safari/605.1.15',
	'firefox_android': 'Mozilla/5.0 (Android 12; Mobile; rv:68.0) Gecko/68.0 Firefox/94.0'
}

def uagent_header_parse(header_value):
	try:
		if header_value != None:
			user_agent_header = user_agent_headers[header_value]
			return user_agent_header
	except:
		print(f"[{bred}Error{noclr}] User-Agent Not Found")
		options_display = f'''
		{bred}User-Agent{noclr} options
		- googlebot

		- chrome_win
		- chrome_linux
		- chrome_iphone
		- chrome_mac
		- chrome_android

		Similarly for {bgreen}firefox{noclr} also add the OS name as shown above
		- firefox_*OS*
		\n'''
		print(options_display)
		sys.exit(0)

def parse_url(url):
	if url[-1] == "/":
		url = url[:(len(url) - 1)]
		return url
	else:
		return url

def parse_links(links_list, target_url, webcontent):
	def parse_css(links_list):
		css_links = []
		for link in links_list:
			if '.css' in fix_http(target_url, link):
				css_links.append(link)
				links_list.remove(link)
		return css_links 
	def parse_imgs(links_list):
		imgs_list = []
		for link in links_list:
			if '.ico' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
			elif '.jpg' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
			elif '.svg' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
			elif '.jpeg' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
			elif '.gif' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
			elif '.png' in fix_http(target_url, link):
				imgs_list.append(link)
				links_list.remove(link)
		return imgs_list
	def parse_js(links_list, webcontent):
		js_links = []
		for link in links_list:
			if '.js' in fix_http(target_url, link):
				js_links.append(link)
				links_list.remove(link)
		js_from_src = pyjs.js_search(webcontent)
		js_links += js_from_src		
		return js_links
	def parse_ext(links_list, web_url):
		ext_links = []
		domain_name = re.findall("//(.*?)/", web_url)[0]
		for link in links_list:
			if not domain_name in fix_http(target_url, link):
				ext_links.append(link)
				links_list.remove(link)
		time.sleep(1)
		link_print('SITE',links_list,web_url)
		return ext_links
	return parse_css(links_list), parse_imgs(links_list), parse_js(links_list,webcontent), parse_ext(links_list,target_url) 


def link_print(type_link, list_links, target_url):
	title = f"\n[{cyan}{type_link}{noclr}] {bgreen}{target_url}{noclr}"
	print(f'''
	{title}
	''')
	for link in list_links:
		link_dia = f'			|---{white}{fix_http(parse_url(target_url), link)}{noclr}'
		print(link_dia)

def link_search(webcontent):
	links_list = re.findall(r'(?i)href=["\']?(.*?)[\s > " \']', webcontent)
	links_list = list(set(links_list))
	return links_list

def check_file_existence(file_path):
	if os.path.exists(file_path) == False:
		return False
	else:
		return True

def fix_http(target_url, link_pgsrc):
	if link_pgsrc[0] == "/":
		link_pgsrc = link_pgsrc[1:len(link_pgsrc)]
	if not 'http' in link_pgsrc:
		url = f"{target_url}/{link_pgsrc}"
		return url
	else:
		return link_pgsrc

def get_response(web_url, user_agent, **proxy_name):
	if web_url[-1] != "/":
		web_url += "/" 
	try:
		headers = requests.utils.default_headers()
		headers.update({'User-Agent': f'{user_agent}'})
		if proxy_name != None:
			print(f"[{bblue}INFO{noclr}] using {bred}proxy{noclr} {cyan}{proxy_name['http']}{noclr}")
			resp = requests.get(web_url,headers=headers, proxies=proxy_name, verify=False, allow_redirects=True)
			links = link_search(resp.text)
			webcontent = resp.text
			css_links, img_links, js_links, ext_links = parse_links(links, web_url, webcontent)
			link_print('EXTERNAL', ext_links, web_url)
			link_print('JS', js_links, web_url)
			link_print('CSS', css_links, web_url)
			link_print('IMG', img_links, web_url)
		else:
			resp = requests.get(web_url,headers=headers, verify=False, allow_redirects=True)
			print(f"[{bblue}INFO{noclr}] using {bred}proxy{noclr} {cyan}{proxy_name['http']}{noclr}")
			links = link_search(resp.text)
			css_links, img_links, js_links, ext_links = parse_links(links, web_url)
			link_print('EXTERNAL', ext_links, web_url)
			link_print('JS', js_links, web_url)
			link_print('CSS', css_links, web_url)
			link_print('IMG', img_links, web_url)
	except Exception as e:
			print(f"[{bred}Error{noclr}] something went wrong !\n")
			print(e)
			sys.exit(0)


parser = optparse.OptionParser("\n./simba [-h or --help] [-f or --file]=<file-path-with-website URLs, one on each line> [-l or --link=URL-of-one-website]")
parser.add_option("-f", "--file", dest="websites_file_path", type='string', help="specify the file containing websites one on each line")
parser.add_option("-l", "--link", dest="website_link", type='string', help="specify a single website link")
parser.add_option("-p", "--proxy", dest="relay_proxy", type='string', help="specify a proxy in following format protocol://<IP>:<port>")
parser.add_option("-u", "--useragent", dest="user_agent", type='string', help="specify a User-Agent Header from options [googlebot,chrome_*win/linux/mac/iphone/android*,firefox_*win/linux/mac/iphone/android*]")
(options, args) = parser.parse_args()

if options.websites_file_path == None and options.website_link == None:
	parser.error(f"\n[{bred}Error{noclr}] Insufficent Arguments supplied {cyan}-f/--file{noclr} or {cyan}-l/--link{noclr} missing value\n")
	sys.exit(0)
elif options.website_link != None:
	website_url = options.website_link
	proxy_relay = options.relay_proxy
	print(f"\n[{bblue}INFO{noclr}] Spidering {cyan}{website_url}{noclr}")
	user_agent_header = uagent_header_parse(options.user_agent)
	get_response(website_url,user_agent_header,http=proxy_relay,https=proxy_relay)
elif check_file_existence(options.websites_file_path):
	file_path = options.websites_file_path
	proxy_relay = options.relay_proxy
	user_agent_header = uagent_header_parse(options.user_agent)
	print(f"\n[{bblue}INFO{noclr}] {cyan}{file_path}{noclr} file found!\n")
	with open(file_path, 'r') as file:
		for line in file.readlines():
			website_url = parse_url(line.strip())
			print(f"\n[{bblue}INFO{noclr}] Spidering {cyan}{website_url}{noclr}")
			get_response(website_url,user_agent_header,http=proxy_relay,https=proxy_relay)


else:
	print(f"\n[{bred}Error{noclr}] something went wrong, please check your {cyan}file path{noclr}\n")



